{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation(x, y, k: int, x_column_names: list = None, y_column_names: list = None, folds_to_return: int = None):\n",
    "        if k <= 0 or k > len(x):\n",
    "            raise ValueError(\n",
    "                \"k must be greater than 0 and less than the number of rows in the dataset\")\n",
    "\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(\n",
    "                \"The number of rows in the dataset must be equal to the number of rows in the expected output\")\n",
    "\n",
    "        if folds_to_return is None:\n",
    "            folds_to_return = k\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        shuffled_dataset = list(zip(x, y))\n",
    "        np.random.shuffle(shuffled_dataset)\n",
    "        x, y = map(\n",
    "            np.array, zip(*shuffled_dataset))\n",
    "\n",
    "        fold_len = int(len(x) / k)\n",
    "        folds = []\n",
    "\n",
    "        # Split the dataset into k folds\n",
    "        # Maybe we want less folds than k\n",
    "        for i in range(min(k, folds_to_return)):\n",
    "            x_test = x[i *\n",
    "                       fold_len: (i + 1) * fold_len]\n",
    "            y_test = y[i *\n",
    "                       fold_len: (i + 1) * fold_len]\n",
    "\n",
    "            x_train = np.concatenate(\n",
    "                [x[:i * fold_len],\n",
    "                 x[(i + 1) * fold_len:]])\n",
    "\n",
    "            y_train = np.concatenate(\n",
    "                [y[:i * fold_len],\n",
    "                 y[(i + 1) * fold_len:]])\n",
    "\n",
    "            # If df_columns is not None, then we need to create a dataframe for both sets\n",
    "            if x_column_names is not None:\n",
    "                x_train = pd.DataFrame(\n",
    "                    x_train, columns=x_column_names)\n",
    "                x_test = pd.DataFrame(\n",
    "                    x_test, columns=x_column_names)\n",
    "                y_train = pd.DataFrame(\n",
    "                    y_train, columns=y_column_names)\n",
    "                y_test = pd.DataFrame(\n",
    "                    y_test, columns=y_column_names)\n",
    "\n",
    "            # Load the test and train sets into the folds\n",
    "            folds.append({\n",
    "                'x_train': x_train,\n",
    "                'y_train': y_train,\n",
    "                'x_test': x_test,\n",
    "                'y_test': y_test,\n",
    "            })\n",
    "\n",
    "        return folds\n",
    "\n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation_eval(x, y, model, k: int, x_column_names: list = None, y_column_names: list = None):\n",
    "        if model is None:\n",
    "            raise ValueError(\"Model cannot be None\")\n",
    "\n",
    "        folds = Metrics.k_fold_cross_validation(\n",
    "            x, y, k=k, x_column_names=x_column_names, y_column_names=y_column_names)\n",
    "\n",
    "        # Evaluate the model on each fold\n",
    "        results = []\n",
    "        errors = []\n",
    "        k_metrics_per_class = {\n",
    "            'accuracy': {label: [] for label in model.classes},\n",
    "            'precision': {label: [] for label in model.classes},\n",
    "            'recall': {label: [] for label in model.classes},\n",
    "            'f1-score': {label: [] for label in model.classes},\n",
    "            'tp-rate': {label: [] for label in model.classes},\n",
    "            'fp-rate': {label: [] for label in model.classes},\n",
    "        }\n",
    "\n",
    "        for fold in folds:\n",
    "            # Train the model\n",
    "            x_train = fold['x_train']\n",
    "            y_train = fold['y_train']\n",
    "            model.train(pd.concat([x_train, y_train], axis=1))\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            x_test = fold['x_test']\n",
    "            y_test = fold['y_test']\n",
    "            predictions = model.test(pd.concat([x_test, y_test], axis=1))\n",
    "\n",
    "            # Compute the metrics\n",
    "            cf_matrix = Metrics.get_confusion_matrix(\n",
    "                predictions[model.classes_column_name].values.tolist(), predictions[model.predicted_class_column_name].values.tolist(), labels=model.classes)\n",
    "            \n",
    "            metrics_df = Metrics.get_metrics_per_class(cf_matrix)[1]\n",
    "\n",
    "            # Push the metrics to the total metrics\n",
    "            for metric_label in k_metrics_per_class:\n",
    "                for label in model.classes:\n",
    "                    k_metrics_per_class[metric_label][label].append(metrics_df[metric_label][label])\n",
    "\n",
    "            results.append(predictions)\n",
    "            errors.append(Metrics.error(predictions, y_test,\n",
    "                          uses_df=x_column_names is not None))\n",
    "\n",
    "        # Compute the average and standard deviation metrics\n",
    "        average_metrics = {metric_label: {label: np.mean(k_metrics_per_class[metric_label][label]) for label in model.classes} for metric_label in k_metrics_per_class}\n",
    "        std_metrics = {metric_label: {label: np.std(k_metrics_per_class[metric_label][label]) for label in model.classes} for metric_label in k_metrics_per_class}\n",
    "                \n",
    "\n",
    "        return results, errors, k_metrics_per_class, average_metrics, std_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def error(predictions, y, uses_df=False):\n",
    "        if uses_df:\n",
    "            predictions = predictions.values\n",
    "            y = y.values\n",
    "\n",
    "        return functools.reduce(\n",
    "            lambda x, y: 0 if x == y else 1, list(zip(predictions, y)), 0) / len(predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_confusion_matrix(y, y_pred, labels=None) -> pd.DataFrame:\n",
    "        cf_matrix = confusion_matrix(y, y_pred, labels=labels)\n",
    "        return pd.DataFrame(cf_matrix, index=labels, columns=labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix_heatmap(cf_matrix, predicted_title=\"Predicted label\", actual_title=\"Truth label\", plot_title=\"\"):\n",
    "        ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "        ax.set_xlabel(predicted_title, fontsize=15)\n",
    "        ax.set_ylabel(actual_title, fontsize=15)\n",
    "        plt.yticks(rotation=0)\n",
    "        ax.xaxis.tick_top()  # x axis on top\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "        plt.title(plot_title, fontsize=20)\n",
    "        plt.show()\n",
    "\n",
    "    # https://arxiv.org/pdf/2008.05756#:~:text=Accuracy%20is%20one%20of%20the,computed%20from%20the%20confusion%20matrix.&text=The%20formula%20of%20the%20Accuracy,confusion%20matrix%20at%20the%20denominator.\n",
    "    @staticmethod\n",
    "    def get_tp_for_class(cf_matrix, label):\n",
    "        return cf_matrix[label][label]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tn_for_class(cf_matrix, label):\n",
    "        # Sum all values and subtract other metrics\n",
    "        return cf_matrix.to_numpy().sum() - \\\n",
    "            (Metrics.get_tp_for_class(cf_matrix, label) +\n",
    "             Metrics.get_fp_for_class(cf_matrix, label) +\n",
    "             Metrics.get_fn_for_class(cf_matrix, label))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fp_for_class(cf_matrix, label):\n",
    "        # Sum all rows except the label row\n",
    "        return sum(cf_matrix.loc[:, label]) - cf_matrix[label][label]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fn_for_class(cf_matrix, label):\n",
    "        # Sum all columns except the label column\n",
    "        return sum(cf_matrix.loc[label, :]) - cf_matrix[label][label]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_accuracy_for_class(cf_matrix, label):\n",
    "        # (TP + TN) / (TP + TN + FP + FN)\n",
    "        numerator = Metrics.get_tp_for_class(\n",
    "            cf_matrix, label) + Metrics.get_tn_for_class(cf_matrix, label)\n",
    "        denominator = numerator + \\\n",
    "            Metrics.get_fp_for_class(cf_matrix, label) + \\\n",
    "            Metrics.get_fn_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_precision_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FP)\n",
    "        numerator = Metrics.get_tp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_fp_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recall_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FN)\n",
    "        numerator = Metrics.get_tp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_fn_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_f1_score_for_class(cf_matrix, label):\n",
    "        # 2 * Precision * Recall / (Precision + Recall)\n",
    "        precision = Metrics.get_precision_for_class(cf_matrix, label)\n",
    "        recall = Metrics.get_recall_for_class(cf_matrix, label)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tp_rate_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FN)\n",
    "        return Metrics.get_recall_for_class(cf_matrix, label)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fp_rate_for_class(cf_matrix, label):\n",
    "        # FP / (FP + TN)\n",
    "        numerator = Metrics.get_fp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_tn_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metrics_per_class(cf_matrix) -> Tuple[dict, pd.DataFrame]:\n",
    "        metrics = {}\n",
    "        for label in cf_matrix.columns:\n",
    "            metrics[label] = {\n",
    "                'accuracy': Metrics.get_accuracy_for_class(cf_matrix, label),\n",
    "                'precision': Metrics.get_precision_for_class(cf_matrix, label),\n",
    "                'recall': Metrics.get_recall_for_class(cf_matrix, label),\n",
    "                'f1-score': Metrics.get_f1_score_for_class(cf_matrix, label),\n",
    "                'tp-rate': Metrics.get_tp_rate_for_class(cf_matrix, label),\n",
    "                'fp-rate': Metrics.get_fp_rate_for_class(cf_matrix, label)\n",
    "            }\n",
    "\n",
    "        # Build a dataframe from the metrics dictionary\n",
    "        df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "        return metrics, df\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_roc_confusion_matrix_for_class(model, x, y, label, threshold):\n",
    "        y_expected = []\n",
    "        y_predicted = []\n",
    "        for x_sample, y_sample in zip(x, y):\n",
    "            # Classify sample to True or False depending on threshold\n",
    "            y_pred_class = Metrics.get_prediction_for_class_with_threshold(\n",
    "                model, x_sample, label, threshold)\n",
    "\n",
    "            # Get y sample class as True or False\n",
    "            y_sample_class = str(y_sample == label)\n",
    "\n",
    "            # Add to arrays\n",
    "            y_expected.append(y_sample_class)\n",
    "            y_predicted.append(y_pred_class)\n",
    "\n",
    "        # Get confusion matrix\n",
    "        cf_matrix = Metrics.get_confusion_matrix(\n",
    "            y_expected, y_predicted, labels=[\"True\", \"False\"])\n",
    "        return cf_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prediction_for_class_with_threshold(model, x, label, threshold):\n",
    "        y_pred = model.classify(x, label=label)\n",
    "        # If prediction is above threshold, return True\n",
    "        return str(y_pred >= threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tp_and_fp_rate_cf_matrix_by_class(cf_matrix, label) -> Tuple[float, float]:\n",
    "        tp = Metrics.get_tp_rate_for_class(cf_matrix, label)\n",
    "        fp = Metrics.get_fp_rate_for_class(cf_matrix, label)\n",
    "        return tp, fp\n",
    "\n",
    "    @staticmethod\n",
    "    def get_roc_curve_for_class(model, x, y, label, thresholds: list[float]) -> Tuple[list[float], list[float]]:\n",
    "        y_tp_rates = []\n",
    "        x_fp_rates = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            cf_matrix = Metrics.get_roc_confusion_matrix_for_class(\n",
    "                model, x, y, label, threshold)\n",
    "            tp, fp = Metrics.get_tp_and_fp_rate_cf_matrix_by_class(\n",
    "                cf_matrix, \"True\")\n",
    "            y_tp_rates.append(tp)\n",
    "            x_fp_rates.append(fp)\n",
    "\n",
    "        return x_fp_rates, y_tp_rates\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(model, x, y, labels: list[str], thresholds: list[float]) -> list[pd.DataFrame]:\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "\n",
    "        roc_curves_df = []\n",
    "\n",
    "        for label in labels:\n",
    "            x_fp_rates, y_tp_rates = Metrics.get_roc_curve_for_class(\n",
    "                model, x, y, label, thresholds)\n",
    "\n",
    "            # Plot a smooth ROC curve\n",
    "            auc_score = Metrics.get_roc_auc_score(x_fp_rates, y_tp_rates)\n",
    "            plt.plot(x_fp_rates, y_tp_rates,\n",
    "                     label=f'{label} - AUC {auc_score:.5f}', marker='o')\n",
    "\n",
    "            # Add to dataframe\n",
    "            roc_curves_df.append(pd.DataFrame([x_fp_rates, y_tp_rates], index=[\n",
    "                                 'FP', 'TP'], columns=thresholds))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.xlabel('FP Rate', fontsize=20)\n",
    "        plt.ylabel('TP Rate', fontsize=20)\n",
    "        plt.show()\n",
    "\n",
    "        return roc_curves_df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_roc_auc_score(x_fp_rates, y_tp_rates):\n",
    "        return auc(x_fp_rates, y_tp_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name, parents, states):\n",
    "        self.name = name\n",
    "        self.parents = parents\n",
    "        self.states = states\n",
    "\n",
    "    def set_probabilities_table(self, probabilities_table):\n",
    "        self.probabilities_table = probabilities_table\n",
    "\n",
    "def get_child_probabilities_table(data_df, parents_node_names, child_node_name):\n",
    "    return data_df.groupby(parents_node_names + [child_node_name]).size()/ len(data_df.groupby(parents_node_names + [child_node_name]).size())\n",
    "\n",
    "def categorize_column(data_df, column_name, categories):\n",
    "    for index, category in enumerate(categories[:-1]):\n",
    "        data_df.loc[(data_df[column_name] >= category[\"floor\"]) & (\n",
    "            data_df[column_name] < category[\"ceiling\"]), column_name + ' category'] = str(index)\n",
    "\n",
    "    data_df.loc[data_df[column_name] >=\n",
    "                            categories[-1][\"floor\"], column_name + ' category'] = str(len(categories)-1)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_simple_probabilities_table(data_df, node):\n",
    "    # el nodo tiene un unico padre\n",
    "    groupby_df = data_df.groupby(list(map(lambda n: n.name, node.parents))+[node.name]).size().reset_index(name=\"appearances\")\n",
    "    groupby_df[node.name] = groupby_df[node.name].astype(int) # no puedo evitar esto\n",
    "    probabilities_table = pd.DataFrame(columns=[node.parents[0].name, node.name, \"frequency\"])\n",
    "    possible_states = list(itertools.product(node.parents[0].states, node.states)) \n",
    "    #para que el estado del nodo actual este al final lo agrego ultimo\n",
    "    \n",
    "    for i, state in enumerate(possible_states):\n",
    "        try:\n",
    "            parent_state_appearances_df = groupby_df.loc[(groupby_df[node.parents[0].name] == state[0])]\n",
    "            parent_state_appearances = parent_state_appearances_df[\"appearances\"].sum()\n",
    "        except: parent_state_appearances = 0\n",
    "\n",
    "        try:\n",
    "            node_state_appearances = parent_state_appearances_df.loc[(parent_state_appearances_df[node.name] == state[1])][\"appearances\"].values[0]\n",
    "        except: node_state_appearances = 0\n",
    "\n",
    "        probabilities_table.loc[i] =  list(state)+[(node_state_appearances+1)/(parent_state_appearances+len(node.states))]\n",
    "\n",
    "    #print(probabilities_table)\n",
    "    return probabilities_table\n",
    "\n",
    "def get_3parents_probabilities_table(data_df, node):\n",
    "    groupby_df = data_df.groupby(list(map(lambda n: n.name, node.parents))+[node.name]).size().reset_index(name=\"appearances\")\n",
    "    groupby_df[node.name] = groupby_df[node.name].astype(int) # no puedo evitar esto\n",
    "    groupby_df[node.parents[0].name] = groupby_df[node.parents[0].name].astype(int) # no puedo evitar esto\n",
    "    groupby_df[node.parents[1].name] = groupby_df[node.parents[1].name].astype(int) # no puedo evitar esto\n",
    "    groupby_df[node.parents[2].name] = groupby_df[node.parents[2].name].astype(int) # no puedo evitar esto\n",
    "    \n",
    "    probabilities_table = pd.DataFrame(columns=list(map(lambda n: n.name, node.parents)) + [node.name, \"frequency\"])\n",
    "    parents_states = list(map(lambda x: x.states, node.parents))\n",
    "    possible_states = list(itertools.product(*parents_states, node.states)) \n",
    "    #para que el estado del nodo actual este al final lo agrego ultimo\n",
    "\n",
    "    for i, state in enumerate(possible_states):\n",
    "\n",
    "        try:\n",
    "            parent_state_appearances_df = groupby_df.loc[(groupby_df[node.parents[0].name] == state[0])& \\\n",
    "            (groupby_df[node.parents[1].name] == state[1]) & \\\n",
    "            (groupby_df[node.parents[2].name] == state[2])]\n",
    "            parent_state_appearances = parent_state_appearances_df[\"appearances\"].sum()\n",
    "        except: parent_state_appearances = 0\n",
    "\n",
    "        try:\n",
    "            node_state_appearances = parent_state_appearances_df.loc[(parent_state_appearances_df[node.name] == state[3])][\"appearances\"].values[0]\n",
    "        except: node_state_appearances = 0\n",
    "        \n",
    "        probabilities_table.loc[i] =  list(state)+[(node_state_appearances+1)/(parent_state_appearances+len(node.states))]\n",
    "\n",
    "    #print(probabilities_table)\n",
    "    return probabilities_table\n",
    "\n",
    "def classify_complete_sample(sample, leaf_node):\n",
    "    frequencies_df = leaf_node.probabilities_table.loc[\n",
    "            (leaf_node.probabilities_table[\"rank\"] == sample[0]) & \\\n",
    "            (leaf_node.probabilities_table[\"gre category\"] == sample[1]) & \\\n",
    "            (leaf_node.probabilities_table[\"gpa category\"] == sample[2])\n",
    "            ]\n",
    "\n",
    "    return frequencies_df.loc[frequencies_df[\"frequency\"].idxmax()][\"admit\"], frequencies_df.loc[frequencies_df[\"frequency\"].idxmax()][\"frequency\"]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./binary.csv\", header=0)\n",
    "GRE_CATEGORY =[{\n",
    "    \"floor\": 0,\n",
    "    \"ceiling\": 500\n",
    "},{\n",
    "    \"floor\": 500,\n",
    "}]\n",
    "data_df = categorize_column(data_df, \"gre\", GRE_CATEGORY)\n",
    "GPA_CATEGORY =[{\n",
    "    \"floor\": 0,\n",
    "    \"ceiling\": 3\n",
    "},{\n",
    "    \"floor\": 3,\n",
    "}]\n",
    "data_df = categorize_column(data_df, \"gpa\", GPA_CATEGORY)\n",
    "\n",
    "folds = Metrics.k_fold_cross_validation(data_df.values.tolist(), data_df[\"admit\"].values.tolist(),\\\n",
    "     x_column_names = data_df.columns, y_column_names= [\"admit\"], k=10, folds_to_return=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PARA METRICAS!\n",
    "data_df = folds[0][\"x_train\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Caso root: RANK\n",
    "ROOT_CATEGORY = [1,2,3,4]\n",
    "ranks_count_df = data_df.groupby([\"rank\"]).size().reset_index(name=\"appearances\")\n",
    "ranks_count_df[\"rank\"] = ranks_count_df[\"rank\"].astype(int) # no puedo evitar esto\n",
    "ranks_frequency = pd.DataFrame(columns=[\"rank\", \"frequency\"])\n",
    "for index, rank in enumerate(ROOT_CATEGORY):\n",
    "    ranks_frequency.loc[index] = \\\n",
    "        [rank, ranks_count_df.loc[ranks_count_df[\"rank\"] == rank][\"appearances\"].values[0]/len(data_df)]\n",
    "\n",
    "root_node = Node(\"rank\", None, ROOT_CATEGORY)\n",
    "#print(ranks_frequency)\n",
    "root_node.set_probabilities_table(ranks_frequency)\n",
    "\n",
    "# Caso 1: GRE\n",
    "gre_node = Node(\"gre category\", [root_node], list(range(len(GRE_CATEGORY))))\n",
    "\n",
    "gre_frequency = get_simple_probabilities_table(data_df, gre_node)\n",
    "gre_node.set_probabilities_table(gre_frequency)\n",
    "\n",
    "# Caso 2: GPA\n",
    "gpa_node = Node(\"gpa category\", [root_node], list(range(len(GPA_CATEGORY))))\n",
    "gpa_frequency = get_simple_probabilities_table(data_df, gpa_node)\n",
    "gpa_node.set_probabilities_table(gpa_frequency)\n",
    "\n",
    "admit_node = Node(\"admit\", [root_node, gre_node, gpa_node], [0,1])\n",
    "admit_frequency = get_3parents_probabilities_table(data_df, admit_node)\n",
    "admit_node.set_probabilities_table(admit_frequency)\n",
    "#print(admit_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Calcular la probabilidad de que una persona que proviene de una escuela con rango 1 no haya sido admitida en la universidad.\n",
    "numerator = 0\n",
    "for i, gpa_state in enumerate(gpa_node.states):\n",
    "    for j, gre_state in enumerate(gre_node.states):\n",
    "        # Usando teorema de la factorizacion de la probabilidad\n",
    "        numerator += \\\n",
    "            admit_node.probabilities_table.loc[\n",
    "            (admit_node.probabilities_table[\"rank\"] == 1) & \\\n",
    "            (admit_node.probabilities_table[\"gpa category\"] == gpa_state) & \\\n",
    "            (admit_node.probabilities_table[\"gre category\"] == gre_state) & \\\n",
    "            (admit_node.probabilities_table[\"admit\"] == 0)][\"frequency\"].values[0] * \\\n",
    "                \\\n",
    "            gpa_node.probabilities_table.loc[\n",
    "            (gpa_node.probabilities_table[\"rank\"] == 1) & \\\n",
    "                (gpa_node.probabilities_table[\"gpa category\"] == gpa_state)][\"frequency\"].values[0] * \\\n",
    "                \\\n",
    "            gre_node.probabilities_table.loc[\n",
    "            (gre_node.probabilities_table[\"rank\"] == 1) & \\\n",
    "                (gre_node.probabilities_table[\"gre category\"] == gre_state)][\"frequency\"].values[0] * \\\n",
    "                \\\n",
    "            root_node.probabilities_table.loc[\n",
    "            (root_node.probabilities_table[\"rank\"] == 1)][\"frequency\"].values[0]\n",
    "\n",
    "denominator = root_node.probabilities_table.loc[\n",
    "            (root_node.probabilities_table[\"rank\"] == 1)]['frequency'][0]\n",
    "            \n",
    "print(\"prob no admitido: \", numerator/denominator)\n",
    "\n",
    "# b) Calcular la probabilidad de que una persona que proviene de una escuela con rango 2, GPA 1 y GRE 0 haya sido admitida en la universidad.\n",
    "prob_admitted = classify_complete_sample([2,1,0], admit_node)\n",
    "print(\"prob admitido: \", prob_admitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test data\n",
    "y_pred = []\n",
    "for row_idx in range(len(folds[0][\"x_test\"])):\n",
    "    row = folds[0][\"x_test\"].iloc[[row_idx]]\n",
    "    sample = list(map(int,[row[\"rank\"].values[0],\\\n",
    "        row[\"gre category\"].values[0],\\\n",
    "            row[\"gpa category\"].values[0]]))\n",
    "    ans = classify_complete_sample(sample, admit_node)\n",
    "    y_pred.append(ans[0])\n",
    "\n",
    "cf_matrix = Metrics.get_confusion_matrix(folds[0][\"y_test\"].values.tolist(), y_pred, [0,1])\n",
    "print(\"Accuracy: \", cf_matrix)\n",
    "heat_map = Metrics.plot_confusion_matrix_heatmap(cf_matrix)\n",
    "\n",
    "df = Metrics.get_metrics_per_class(cf_matrix)[1]\n",
    "df.to_csv(\"./dump/metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
