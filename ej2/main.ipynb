{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation(x, y, k: int, x_column_names: list = None, y_column_names: list = None, folds_to_return: int = None):\n",
    "        if k <= 0 or k > len(x):\n",
    "            raise ValueError(\n",
    "                \"k must be greater than 0 and less than the number of rows in the dataset\")\n",
    "\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(\n",
    "                \"The number of rows in the dataset must be equal to the number of rows in the expected output\")\n",
    "\n",
    "        if folds_to_return is None: \n",
    "            folds_to_return = k\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        shuffled_dataset = list(zip(x, y))\n",
    "        np.random.shuffle(shuffled_dataset)\n",
    "        x, y = map(\n",
    "            np.array, zip(*shuffled_dataset))\n",
    "\n",
    "        fold_len = int(len(x) / k)\n",
    "        folds = []\n",
    "\n",
    "        # Split the dataset into k folds\n",
    "        # Maybe we want less folds than k\n",
    "        for i in range(min(k, folds_to_return)):\n",
    "            x_test = x[i *\n",
    "                       fold_len: (i + 1) * fold_len]\n",
    "            y_test = y[i *\n",
    "                       fold_len: (i + 1) * fold_len]\n",
    "\n",
    "            x_train = np.concatenate(\n",
    "                [x[:i * fold_len],\n",
    "                 x[(i + 1) * fold_len:]])\n",
    "\n",
    "            y_train = np.concatenate(\n",
    "                [y[:i * fold_len],\n",
    "                 y[(i + 1) * fold_len:]])\n",
    "\n",
    "            # If df_columns is not None, then we need to create a dataframe for both sets\n",
    "            if x_column_names is not None:\n",
    "                x_train = pd.DataFrame(\n",
    "                    x_train, columns=x_column_names)\n",
    "                x_test = pd.DataFrame(\n",
    "                    x_test, columns=x_column_names)\n",
    "                y_train = pd.DataFrame(\n",
    "                    y_train, columns=y_column_names)\n",
    "                y_test = pd.DataFrame(\n",
    "                    y_test, columns=y_column_names)\n",
    "\n",
    "            # Load the test and train sets into the folds\n",
    "            folds.append({\n",
    "                'x_train': x_train,\n",
    "                'y_train': y_train,\n",
    "                'x_test': x_test,\n",
    "                'y_test': y_test,\n",
    "            })\n",
    "\n",
    "        return folds\n",
    "\n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation_eval(x, y, model, k: int, x_column_names: list = None, y_column_names: list = None):\n",
    "        if model is None:\n",
    "            raise ValueError(\"Model cannot be None\")\n",
    "\n",
    "        folds = Metrics.k_fold_cross_validation(\n",
    "            x, y, k=k, x_column_names=x_column_names, y_column_names=y_column_names)\n",
    "\n",
    "        # Evaluate the model on each fold\n",
    "        results = []\n",
    "        errors = []\n",
    "        for fold in folds:\n",
    "            # Train the model\n",
    "            x_train = fold['x_train']\n",
    "            y_train = fold['y_train']\n",
    "            model.train(pd.concat([x_train, y_train], axis=1))\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            x_test = fold['x_test']\n",
    "            y_test = fold['y_test']\n",
    "            predictions = model.test(pd.concat([x_test, y_test], axis=1))\n",
    "            results.append(predictions)\n",
    "            errors.append(Metrics.error(predictions, y_test, uses_df=x_column_names is not None))\n",
    "\n",
    "        return results, errors\n",
    "\n",
    "    @staticmethod\n",
    "    def error(predictions, y, uses_df=False):\n",
    "        if uses_df:\n",
    "            predictions = predictions.values\n",
    "            y = y.values\n",
    "\n",
    "        return functools.reduce(\n",
    "            lambda x, y: 0 if x == y else 1, list(zip(predictions, y)), 0) / len(predictions)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_confusion_matrix(y, y_pred, labels=None) -> pd.DataFrame:\n",
    "        cf_matrix = confusion_matrix(y, y_pred, labels=labels)\n",
    "        return pd.DataFrame(cf_matrix, index=labels, columns=labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix_heatmap(cf_matrix):\n",
    "        ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='d' )\n",
    "        ax.set_xlabel('Predicted labels', fontsize=18)\n",
    "        ax.set_ylabel('True labels', fontsize=18)\n",
    "        plt.yticks(rotation=0)\n",
    "        ax.xaxis.tick_top() # x axis on top\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "        plt.show()\n",
    "\n",
    "    # https://arxiv.org/pdf/2008.05756#:~:text=Accuracy%20is%20one%20of%20the,computed%20from%20the%20confusion%20matrix.&text=The%20formula%20of%20the%20Accuracy,confusion%20matrix%20at%20the%20denominator.\n",
    "    @staticmethod\n",
    "    def get_tp_for_class(cf_matrix, label):\n",
    "        return cf_matrix[label][label]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tn_for_class(cf_matrix, label):\n",
    "        # Sum all values and subtract other metrics\n",
    "        return cf_matrix.to_numpy().sum() - \\\n",
    "            (Metrics.get_tp_for_class(cf_matrix, label) + \\\n",
    "            Metrics.get_fp_for_class(cf_matrix, label) + \\\n",
    "            Metrics.get_fn_for_class(cf_matrix, label))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fp_for_class(cf_matrix, label):\n",
    "        # Sum all rows except the label row\n",
    "        return sum(cf_matrix.loc[:, label]) - cf_matrix[label][label]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_fn_for_class(cf_matrix, label):\n",
    "        # Sum all columns except the label column\n",
    "        return sum(cf_matrix.loc[label, :]) - cf_matrix[label][label]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_accuracy_for_class(cf_matrix, label):\n",
    "        # (TP + TN) / (TP + TN + FP + FN)\n",
    "        numerator = Metrics.get_tp_for_class(cf_matrix, label) + Metrics.get_tn_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_fp_for_class(cf_matrix, label) + Metrics.get_fn_for_class(cf_matrix, label) \n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_precision_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FP)\n",
    "        numerator = Metrics.get_tp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_fp_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recall_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FN)\n",
    "        numerator = Metrics.get_tp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_fn_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_f1_score_for_class(cf_matrix, label):\n",
    "        # 2 * Precision * Recall / (Precision + Recall)\n",
    "        precision = Metrics.get_precision_for_class(cf_matrix, label)\n",
    "        recall = Metrics.get_recall_for_class(cf_matrix, label)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tp_rate_for_class(cf_matrix, label):\n",
    "        # TP / (TP + FN)\n",
    "        return Metrics.get_recall_for_class(cf_matrix, label)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fp_rate_for_class(cf_matrix, label):\n",
    "        # FP / (FP + TN)\n",
    "        numerator = Metrics.get_fp_for_class(cf_matrix, label)\n",
    "        denominator = numerator + Metrics.get_tn_for_class(cf_matrix, label)\n",
    "        return numerator / denominator\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metrics_per_class(cf_matrix):\n",
    "        metrics = {}\n",
    "        for label in cf_matrix.columns:\n",
    "            metrics[label] = {\n",
    "                'accuracy': Metrics.get_accuracy_for_class(cf_matrix, label),\n",
    "                'precision': Metrics.get_precision_for_class(cf_matrix, label),\n",
    "                'recall': Metrics.get_recall_for_class(cf_matrix, label),\n",
    "                'f1-score': Metrics.get_f1_score_for_class(cf_matrix, label),\n",
    "                'tp-rate': Metrics.get_tp_rate_for_class(cf_matrix, label),\n",
    "                'fp-rate': Metrics.get_fp_rate_for_class(cf_matrix, label)\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def get_roc_confusion_matrix_for_class(model, x, y, label, threshold):\n",
    "        y_expected = []\n",
    "        y_predicted = []\n",
    "        for x_sample, y_sample in zip(x, y):\n",
    "            # Classify sample to True or False depending on threshold\n",
    "            y_pred_class = Metrics.get_prediction_for_class_with_threshold(model, x_sample, label, threshold)\n",
    "\n",
    "            # Get y sample class as True or False\n",
    "            y_sample_class = str(y_sample == label)\n",
    "\n",
    "            # Add to arrays\n",
    "            y_expected.append(y_sample_class)\n",
    "            y_predicted.append(y_pred_class)\n",
    "\n",
    "        # Get confusion matrix\n",
    "        cf_matrix = Metrics.get_confusion_matrix(y_expected, y_predicted, labels=[\"True\", \"False\"])\n",
    "        return cf_matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prediction_for_class_with_threshold(model, x, label, threshold):\n",
    "        y_pred = model.classify(x, label=label)\n",
    "        # If prediction is above threshold, return True\n",
    "        return str(y_pred >= threshold)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tp_and_fp_rate_cf_matrix_by_class(cf_matrix, label) -> Tuple[float, float]:\n",
    "        tp = Metrics.get_tp_rate_for_class(cf_matrix, label)\n",
    "        fp = Metrics.get_fp_rate_for_class(cf_matrix, label)\n",
    "        return tp, fp\n",
    "\n",
    "    @staticmethod\n",
    "    def get_roc_curve_for_class(model, x, y, label, thresholds: list[float]) -> Tuple[list[float], list[float]]:\n",
    "        y_tp_rates = []\n",
    "        x_fp_rates = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            cf_matrix = Metrics.get_roc_confusion_matrix_for_class(model, x, y, label, threshold)\n",
    "            tp, fp = Metrics.get_tp_and_fp_rate_cf_matrix_by_class(cf_matrix, \"True\")\n",
    "            y_tp_rates.append(tp)\n",
    "            x_fp_rates.append(fp)\n",
    "        \n",
    "        return x_fp_rates, y_tp_rates\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_roc_curves(model, x, y, labels: list[str], thresholds: list[float]) -> list[pd.DataFrame]:\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        \n",
    "        roc_curves_df = []\n",
    "\n",
    "        for label in labels:\n",
    "            x_fp_rates, y_tp_rates = Metrics.get_roc_curve_for_class(model, x, y, label, thresholds)\n",
    "            \n",
    "            # Plot a smooth ROC curve\n",
    "            auc_score = Metrics.get_roc_auc_score(x_fp_rates, y_tp_rates)\n",
    "            plt.plot(x_fp_rates, y_tp_rates, label=f'{label} - AUC {auc_score}', marker='o')\n",
    "\n",
    "            # Add to dataframe\n",
    "            roc_curves_df.append(pd.DataFrame([x_fp_rates, y_tp_rates], index=['FP', 'TP'], columns=thresholds))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.show()\n",
    "\n",
    "        return roc_curves_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_roc_auc_score(x_fp_rates, y_tp_rates):\n",
    "        return auc(x_fp_rates, y_tp_rates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesTextClassifier():\n",
    "    predicted_class_column_name = 'prediccion'\n",
    "\n",
    "    def __init__(self, classes, text_column_name, classes_column_name, tokenizer):\n",
    "        self.relative_frequencies = {}  # by_class_by_value\n",
    "        self.negated_relative_accum_frequencies = {}  # by_class\n",
    "        self.classes_probabilities = {}\n",
    "        self.classes = classes\n",
    "        self.classes_column_name = classes_column_name\n",
    "        self.text_column_name = text_column_name\n",
    "        self.row_count_by_class = {}\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, data_df):\n",
    "        word_list = set()\n",
    "        for c in self.classes:\n",
    "            class_df = data_df[data_df[self.classes_column_name] == c]\n",
    "\n",
    "            self.row_count_by_class[c] = len(class_df)\n",
    "            self.classes_probabilities[c] = len(class_df) / len(data_df)\n",
    "\n",
    "            values_appearances = {}\n",
    "            # iterate over rows within class\n",
    "            for i in range(len(class_df)):\n",
    "                row = class_df.iloc[[i]]\n",
    "\n",
    "                tokenized_text = self.tokenizer(\n",
    "                    row[self.text_column_name].values[0])\n",
    "                # iterate over values within row\n",
    "                for token in tokenized_text:\n",
    "                    word_list.add(token)\n",
    "                    # initialize possible value if not present in map, otherwise increment appereances\n",
    "                    if token not in values_appearances:\n",
    "                        values_appearances[token] = 1\n",
    "                    else:\n",
    "                        values_appearances[token] += 1\n",
    "\n",
    "            # calculate relative frequencies\n",
    "            self.relative_frequencies[c] = {token: (token_count + 1) / (len(class_df) + len(\n",
    "                self.classes)) for token, token_count in values_appearances.items()}\n",
    "        \n",
    "        # For every class, calculate the relative accumulative frequencies of the words not being in it\n",
    "        for c in self.classes: \n",
    "            class_df = data_df[data_df[self.classes_column_name] == c]\n",
    "            p = 1\n",
    "            for word in word_list:\n",
    "                if word in self.relative_frequencies[c]:\n",
    "                    # If the word is in the class, just negate it\n",
    "                    p *= (1 - self.relative_frequencies[c][word])\n",
    "                else:\n",
    "                    # If the word is not in the class, then negate the laplace smoothing\n",
    "                    p *= (1 - (1 / (len(class_df) + len(self.classes))))\n",
    "            \n",
    "            # Store all the negated relative accumulative frequencies for the class\n",
    "            self.negated_relative_accum_frequencies[c] = p\n",
    "        \n",
    "\n",
    "    def classify(self, sample, label = None):\n",
    "        tokenized_sample = self.tokenizer(sample)\n",
    "        classification = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "\n",
    "            prod = self.classes_probabilities[c]\n",
    "            token_likelihoods = self.relative_frequencies[c]\n",
    "            laplace_constant = 1 / \\\n",
    "                (self.row_count_by_class[c] + len(self.classes))\n",
    "\n",
    "            for token in tokenized_sample:\n",
    "                token_relative_frequency = token_likelihoods[token] if token in token_likelihoods else laplace_constant\n",
    "\n",
    "                # Divide by the probability of not being in the class, as below we are multiplying by it\n",
    "                # so we have to cancel it out\n",
    "                prod *= token_relative_frequency / (1 - token_relative_frequency)\n",
    "\n",
    "            # Multiply by the probability of not being in the class, as every word is an attribute\n",
    "            classification[c] = prod * self.negated_relative_accum_frequencies[c]\n",
    "\n",
    "        # Divide by sum of all class probabilities to get final probability\n",
    "        classification = {c: p / sum(classification.values()) for c, p in classification.items()}\n",
    "        classifications = dict(sorted(classification.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # If a specific label is given, return the probability of the label\n",
    "        if label is not None:\n",
    "            return classifications[label]\n",
    "        return classifications\n",
    "\n",
    "    def test(self, test_df):\n",
    "        predicted_classes = []\n",
    "        expected_classes= []\n",
    "\n",
    "        for i in range(len(test_df)):\n",
    "            row = test_df.iloc[[i]]\n",
    "            row_class = row[self.classes_column_name].values[0]\n",
    "\n",
    "            classification = self.classify(\n",
    "                row[self.text_column_name].values[0])\n",
    "            predicted_class = max(classification, key=classification.get)\n",
    "\n",
    "            predicted_classes.append(predicted_class)\n",
    "            expected_classes.append(row_class)\n",
    "\n",
    "        # append results column to new dataframe\n",
    "        results_df = test_df.copy()\n",
    "        results_df[self.predicted_class_column_name] = predicted_classes\n",
    "        results_df[self.classes_column_name] = expected_classes\n",
    "\n",
    "        return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./Noticias_argentinas.txt\", header=0, sep='\\t')\n",
    "classes = [\"Economia\", \"Salud\", \"Deportes\", \"Ciencia y Tecnologia\"]\n",
    "\n",
    "data_df = data_df[data_df[\"categoria\"].isin(classes)]\n",
    "\n",
    "nbclassifier = NaiveBayesTextClassifier(\n",
    "    classes, \"titular\", \"categoria\", tokenize)\n",
    "\n",
    "y = data_df.loc[:, [\"categoria\"]]\n",
    "x = data_df.loc[:, [\"titular\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "# results, errors = Metrics.k_fold_cross_validation_eval(x.values.tolist(), y.values.tolist(\n",
    "# ), model=nbclassifier, x_column_names=x.columns, y_column_names=y.columns, k=5)\n",
    "# print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test sets\n",
    "folds = Metrics.k_fold_cross_validation(x.values.tolist(), y.values.tolist(), x_column_names=x.columns, y_column_names=y.columns, k=10, folds_to_return=1)\n",
    "train = pd.concat([folds[0]['x_train'], folds[0]['y_train']], axis=1)\n",
    "test = pd.concat([folds[0]['x_test'], folds[0]['y_test']], axis=1)\n",
    "# Train the model\n",
    "nbclassifier.train(train)\n",
    "\n",
    "# Test the model\n",
    "results = nbclassifier.test(test)\n",
    "\n",
    "# train_set = data_df.sample(frac=0.8, random_state=1)\n",
    "# print(\"======================\")\n",
    "# print(train_set.index)\n",
    "# test_set = data_df.drop(train_set.index)\n",
    "\n",
    "# nbclassifier.train(train_set)\n",
    "# print(nbclassifier.test(test_set))\n",
    "\n",
    "# sample_columns = data_df.columns.drop(\"categoria\")\n",
    "\n",
    "# print(nbclassifier.classify(\"Histórico: Los Pumas derrotaron por primera vez a los All Blacks en Nueva Zelanda\"))\n",
    "# print(nbclassifier.classify(\"Maradona negó haber criticado a Messi, disparó otra vez contra Scaloni y también la ligó Solari\"))\n",
    "# print(nbclassifier.classify(\"Guzmán\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix\n",
    "# confusion_matrix = Metrics.get_confusion_matrix(\n",
    "#     results[nbclassifier.classes_column_name].values.tolist(), results[nbclassifier.predicted_class_column_name].values.tolist(), labels=nbclassifier.classes)\n",
    "# Metrics.plot_confusion_matrix_heatmap(confusion_matrix)\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation metrics by class\n",
    "# evaluation_metrics = Metrics.get_metrics_per_class(confusion_matrix)\n",
    "# print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ROC curve for each class\n",
    "thresholds = np.linspace(0, 1, 11, endpoint=True)\n",
    "roc_curves_df = Metrics.plot_roc_curves(nbclassifier, test[nbclassifier.text_column_name].values.tolist(), test[nbclassifier.classes_column_name].values.tolist(), labels=nbclassifier.classes, thresholds=thresholds)\n",
    "for roc_curve_df in roc_curves_df:\n",
    "    print(roc_curve_df)\n",
    "# for label in nbclassifier.classes:\n",
    "#     print(f'Confusion matrix for class {label}')\n",
    "#     print(Metrics.get_roc_confusion_matrix_for_class(nbclassifier, results[nbclassifier.classes_column_name].values.tolist(), results[nbclassifier.predicted_class_column_name].values.tolist(), label, 0.175))\n",
    "#     # print(Metrics.get_roc_confusion_matrix_for_class(nbclassifier, [\"Economia\", \"Economia\"], [\"Economia\", \"Economia\"], label, 0.5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "964b1fa5364468aa142ca826e1bab4686480a96d9d38c308c84d42c2ef7b8b78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
